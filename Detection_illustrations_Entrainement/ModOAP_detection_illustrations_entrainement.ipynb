{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Nanterre_MODOAP_detection_illustrations_entrainement.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1em4rMcjcR_"
      },
      "source": [
        "# ***MODOAP - Detection d'illustrations dans les documents historiques - ENTRAINEMENT***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUR53o2R7IxX"
      },
      "source": [
        "**Détection d'images dans les documents historiques**\n",
        "\n",
        "\n",
        "**Script d'auto apprentissage**\n",
        "\n",
        "Ce script permet d'entraîner l'algorithme Mask-RCNN à la segmentation d'objets sur ses propres données d'entraînement. \n",
        "\n",
        "Il requiert un corpus d'entraînement : un dossier sur un Google Drive composé de deux dossiers \"train\" et \"val\" contenant chacun les images et leur fichier d'annotation.\n",
        "\n",
        "L'annotation du corpus peut être opérée grâce aux outils VIA (https://www.robots.ox.ac.uk/~vgg/software/via/) et Annotate.\n",
        "\n",
        "Le script implémente la configuration décrite sur https://github.com/matterport/Mask_RCNN\n",
        "\n",
        "**Ce script doit impérativement être lancé dans un environnement GPU : Runtime -> Change runtime type -> GPU**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihzf5ih94aWu"
      },
      "source": [
        "## 0. Connexion à un compte Google Drive, création de l'architecture et installation des pré-requis\n",
        "\n",
        "Nécessite de se connecter à son compte Google Drive et d'entrer un code de vérification.\n",
        "\n",
        "Crée un dossier Outils_Modoap sur le Drive qui servira à stocker les poids générés lors de l'apprentissage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5V_iUvSi2g29"
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "if not os.path.exists(\"/content/drive/My Drive\"):\n",
        "  drive.mount('/content/drive')\n",
        "else : print(\"Le Drive est déjà monté\")\n",
        "\n",
        "if not os.path.exists(\"/content/drive/My Drive/Outils_Modoap/Detection_Illustrations\"):\n",
        "  os.makedirs('/content/drive/My Drive/Outils_Modoap/Detection_Illustrations/Poids')\n",
        "\n",
        "%cd\n",
        "if not os.path.exists(\"/root/Mask_RCNN\"):\n",
        "\n",
        "  !git clone --quiet https://github.com/matterport/Mask_RCNN.git\n",
        "  %cd /root/Mask_RCNN\n",
        "  !pip install -q PyDrive\n",
        "  !pip install -r requirements.txt\n",
        "  !python setup.py install\n",
        "  !cp ~/Mask_RCNN/samples/balloon/balloon.py ./illustration.py\n",
        "  !sed -i -- 's/balloon/illustration/g' illustration.py\n",
        "  !sed -i -- 's/Balloon/Illustration/g' illustration.py\n",
        "\n",
        "else : print(\"L'algorithme est déjà téléchargé\")\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "!pip install q keras==2.1.5\n",
        "!pip install q keras==2.1.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8dL8VUMWhUw"
      },
      "source": [
        "# 1. Préparation du corpus d'entraînement\n",
        "\n",
        "- **Pour un corpus annoté avec VIA :**\n",
        "\n",
        "Le dossier du corpus doit être constitué de cette architecture :\n",
        "\n",
        "(le nom et le format des images sont libres)\n",
        "\n",
        "(le fichier via_region_data.json est obtenu dans VIA par Annotation -> Export Annotations (as json)\n",
        "\n",
        "\n",
        "```\n",
        "  train\n",
        "  img1.jpg\n",
        "  img2.jpg\n",
        "  ...\n",
        "  img100.jpg\n",
        "  via_region_data.json\n",
        "val\n",
        "  img101.jpg\n",
        "  img102.jpg\n",
        "  ...\n",
        "  img120.jpg\n",
        "  via_region_data.json\n",
        "```\n",
        "\n",
        "- **Pour un corpus annoté avec Annotate :**\n",
        "\n",
        "Le dossier du corpus doit être constitué de cette architecture:\n",
        "\n",
        "Le nom et le format des images sont libres\n",
        "\n",
        "Le fichier annotate_region_data.csv est obtenu dans Annotate par View and Export results of annotation -> Annotations -> Export CSV -> Use Coma Separator\n",
        "\n",
        "Testé avec Annotate 1.7\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "  train\n",
        "  img1.jpg\n",
        "  img2.jpg\n",
        "  ...\n",
        "  img100.jpg\n",
        "  annotate_region_data.csv\n",
        "val\n",
        "  img101.jpg\n",
        "  img102.jpg\n",
        "  ...\n",
        "  img120.jpg\n",
        "  annotate_region_data.csv\n",
        "```\n",
        "\n",
        "**Spécification du corpus**\n",
        "\n",
        "Entrer le chemin absolu vers le dossier contenant le corpus sur le drive. \n",
        "La racine du Google Drive est */content/drive/My Drive/*\n",
        "\n",
        "Possibilité de copier/coller le chemin depuis la fenêtre de gauche : *Files -> clic droit sur un dossier -> Copy Path*\n",
        "\n",
        "Exemple de chemin:\n",
        "\n",
        "/content/drive/My Drive/Corpus/corpus_pour_detection/ \n",
        "\n",
        "Ou bien entrer \"exemple\" pour télécharger un corpus de démonstration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mn4zkbPOOGzG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "10d2ec25-9d2d-43df-e995-0fcabc0ef648"
      },
      "source": [
        "dossier_corpus = input(\"Entrer le chemin absolu du dossier contenant le corpus, ou taper \\\"exemple\\\" \")\n",
        "if dossier_corpus == \"exemple\" :\n",
        "  if not os.path.exists(\"/content/drive/My Drive/Outils_Modoap/Corpus_demonstrations/corpus_detection_illustration/\"):\n",
        "    os.makedirs('/content/drive/My Drive/Outils_Modoap/Corpus_demonstrations/corpus_detection_illustration/')\n",
        "  %cd /content/drive/My Drive/Outils_Modoap/Corpus_demonstrations/corpus_detection_illustration/\n",
        "  !wget https://github.com/cyril521/modoap-seg/raw/master/Datasets/exemple_entrainement_droitsetlibertes/corpus_train_val.7z.001\n",
        "  !wget https://github.com/cyril521/modoap-seg/raw/master/Datasets/exemple_entrainement_droitsetlibertes/corpus_train_val.7z.002\n",
        "  !wget https://github.com/cyril521/modoap-seg/raw/master/Datasets/exemple_entrainement_droitsetlibertes/corpus_train_val.7z.003\n",
        "\n",
        "  !7z x ./corpus_train_val.7z.001\n",
        "  os.remove(\"/content/drive/My Drive/Outils_Modoap/Corpus_demonstrations/corpus_detection_illustration/corpus_train_val.7z.001\")\n",
        "  os.remove(\"/content/drive/My Drive/Outils_Modoap/Corpus_demonstrations/corpus_detection_illustration/corpus_train_val.7z.002\")\n",
        "  os.remove(\"/content/drive/My Drive/Outils_Modoap/Corpus_demonstrations/corpus_detection_illustration/corpus_train_val.7z.003\")\n",
        "\n",
        "  !7z x ./corpus_entrainement_DL.zip\n",
        "  os.remove(\"/content/drive/My Drive/Outils_Modoap/Corpus_demonstrations/corpus_detection_illustration/corpus_entrainement_DL.zip\")\n",
        "  dossier_corpus = \"/content/drive/My Drive/Outils_Modoap/Corpus_demonstrations/corpus_detection_illustration/\"\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Entrer le chemin absolu du dossier contenant le corpus, ou taper \"exemple\" /content/drive/My Drive/Outils_Modoap/Corpus_demonstrations/corpus_detection_illustration/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hfc_7E1FUvMX"
      },
      "source": [
        "**3.4 Traduction des annotations réalisées avec VIA** (nécessaire)\n",
        "\n",
        "Rappel : les fichiers d'annotations VIA dans les dossiers \"train\" et \"val\" doivent tous deux être nommés \"via_region_data.json\"\n",
        "\n",
        "Cette cellule transforme les annotations rectangulaires en polygones à 4 coordonnées, traitables par l'algorithme"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REgCpCfrSlys"
      },
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "\n",
        "def traductionVIA(viajson):\n",
        "  annotations = json.load(open(viajson))\n",
        "  if \"_via_settings\" in annotations.keys() :\n",
        "      annotations_new = {}\n",
        "      for image in annotations[\"_via_img_metadata\"]:\n",
        "          valeur_image = annotations[\"_via_img_metadata\"][image]\n",
        "          nom = image\n",
        "          annotations_new[nom] = valeur_image\n",
        "          for i in range(len(annotations[\"_via_img_metadata\"][image][\"regions\"])) :\n",
        "              if annotations[\"_via_img_metadata\"][image][\"regions\"][i][\"shape_attributes\"][\"name\"] == \"rect\":\n",
        "\n",
        "                  x = annotations[\"_via_img_metadata\"][image][\"regions\"][i][\"shape_attributes\"][\"x\"]\n",
        "                  y = annotations[\"_via_img_metadata\"][image][\"regions\"][i][\"shape_attributes\"][\"y\"]\n",
        "                  width = annotations[\"_via_img_metadata\"][image][\"regions\"][i][\"shape_attributes\"][\"width\"]\n",
        "                  height = annotations[\"_via_img_metadata\"][image][\"regions\"][i][\"shape_attributes\"][\"height\"]\n",
        "\n",
        "                  annotations_new[image][\"regions\"][i][\"shape_attributes\"][\"all_points_x\"] = [x, x+width, x+width, x]\n",
        "                  annotations_new[image][\"regions\"][i][\"shape_attributes\"][\"all_points_y\"] = [y, y, y+height, y+height]\n",
        "                  annotations_new[image][\"regions\"][i][\"shape_attributes\"][\"name\"] = \"polygon\"\n",
        "                  del annotations_new[image][\"regions\"][i][\"shape_attributes\"][\"y\"]\n",
        "                  del annotations_new[image][\"regions\"][i][\"shape_attributes\"][\"x\"]\n",
        "                  del annotations_new[image][\"regions\"][i][\"shape_attributes\"][\"width\"]\n",
        "                  del annotations_new[image][\"regions\"][i][\"shape_attributes\"][\"height\"]\n",
        "\n",
        "  else :\n",
        "      annotations_new = annotations\n",
        "      for image in annotations :\n",
        "          for i in range(len(annotations[image][\"regions\"])) :\n",
        "              if annotations[image][\"regions\"][i][\"shape_attributes\"][\"name\"] == \"rect\":\n",
        "                  x = annotations[image][\"regions\"][i][\"shape_attributes\"][\"x\"]\n",
        "                  y = annotations[image][\"regions\"][i][\"shape_attributes\"][\"y\"]\n",
        "                  width = annotations[image][\"regions\"][i][\"shape_attributes\"][\"width\"]\n",
        "                  height = annotations[image][\"regions\"][i][\"shape_attributes\"][\"height\"]\n",
        "\n",
        "                  annotations_new[image][\"regions\"][i][\"shape_attributes\"][\"all_points_x\"] = [x, x+width, x+width, x]\n",
        "                  annotations_new[image][\"regions\"][i][\"shape_attributes\"][\"all_points_y\"] = [y, y, y+height, y+height]\n",
        "                  annotations_new[image][\"regions\"][i][\"shape_attributes\"][\"name\"] = \"polygon\"\n",
        "                  del annotations_new[image][\"regions\"][i][\"shape_attributes\"][\"y\"]\n",
        "                  del annotations_new[image][\"regions\"][i][\"shape_attributes\"][\"x\"]\n",
        "                  del annotations_new[image][\"regions\"][i][\"shape_attributes\"][\"width\"]\n",
        "                  del annotations_new[image][\"regions\"][i][\"shape_attributes\"][\"height\"]\n",
        "              \n",
        "  with open('via_region_data.json', 'w') as json_file:\n",
        "    json.dump(annotations_new, json_file)\n",
        "\n",
        "path = os.path.join(dossier_corpus, \"train\", \"via_region_data.json\")\n",
        "traductionVIA(path)\n",
        "\n",
        "path2 = os.path.join(dossier_corpus, \"val\", \"via_region_data.json\")\n",
        "traductionVIA(path2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6efPqsshW5b6"
      },
      "source": [
        "**3.5 Traduction des annotations réalisées avec Annotate** (necessaire)\n",
        "\n",
        "Rappel : les fichiers d'annotations Annotate dans les dossiers \"train\" et \"val\" doivent tous deux être nommés \"annotate_region_data.csv\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqxnM6ooXNq1"
      },
      "source": [
        "import os \n",
        "import pandas as pd\n",
        "\n",
        "def traductionANNOTATE(annotations_csv):\n",
        "\n",
        "  annotationz_new = {}\n",
        "  annotationz = pd.read_csv(annotations_csv).sort_values('File')\n",
        "\n",
        "  for index, row in annotationz.iterrows() :\n",
        "    filename = row[\"File\"]\n",
        "    size = \"\"\n",
        "    regions = {}\n",
        "    x = int(row[\"X\"])\n",
        "    y = int(row[\"Y\"])\n",
        "    w = int(row[\"W\"])\n",
        "    h = int(row[\"H\"])\n",
        "    all_points_x = [x, x+w, x+w, x]\n",
        "    all_points_y = [y, y, y+h, y+h]\n",
        "      \n",
        "    if filename not in annotationz_new.keys(): \n",
        "      annotationz_new[filename] = {\"filename\":filename, \"size\":size,\"regions\":{0:{\"shape_attributes\":{\"name\":\"polygon\", \"all_points_x\":all_points_x,\"all_points_y\":all_points_y}, \"region_attributes\":{}}}, \"file_attributes\":{}}\n",
        "      \n",
        "    else :\n",
        "      i = max(annotationz_new[filename][\"regions\"].keys()) + 1\n",
        "      annotationz_new[filename][\"regions\"][i] = {\"shape_attributes\":{\"name\":\"polygon\", \"all_points_x\":all_points_x,\"all_points_y\":all_points_y}, \"region_attributes\":{}}\n",
        "\n",
        "  with open('via_region_data.json', 'w') as json_file:\n",
        "    json.dump(annotationz_new, json_file)\n",
        "      \n",
        "path = os.path.join(dossier_corpus, \"train\", \"annotate_region_data.csv\")\n",
        "traductionANNOTATE(path)\n",
        "\n",
        "path2 = os.path.join(dossier_corpus, \"val\", \"annotate_region_data.csv\")\n",
        "traductionANNOTATE(path2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwUCh_B1Bmuk"
      },
      "source": [
        "**4. Préparation et configuration de l'algorithme**\n",
        "\n",
        "Nécessite de préciser le nombre d'époques pour l'apprentissage ( par défaut : 30 )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otF-mtnRPvlU"
      },
      "source": [
        "%cd /root/Mask_RCNN/\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "import skimage.io\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "\n",
        "epoques = input(\"Entrer le nombre d'époques pour l'entraînement (par défaut : 30) : \")\n",
        "\n",
        "!sed -i -- 's/epochs=30/epochs=$epoques/g' illustration.py \n",
        "\n",
        "ROOT_DIR = os.path.abspath(\"/\")\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "sys.path.append(ROOT_DIR)  \n",
        "from mrcnn import utils\n",
        "import mrcnn.model as modellib\n",
        "from mrcnn import visualize\n",
        "import illustration\n",
        "\n",
        "IMAGE_DIR = dossier_corpus\n",
        "\n",
        "config = illustration.IllustrationConfig()\n",
        "\n",
        "class InferenceConfig(config.__class__):\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "\n",
        "config = InferenceConfig()\n",
        "config.display()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFa5GYB3CDbH"
      },
      "source": [
        "**5. Lancement de l'entraînement**\n",
        "\n",
        "L'entraînement peut prendre plusieurs heures en fonction du nombre d'époques choisi. Le résultat est un fichier de poids .h5 sauvegardé dans */content/drive/My Drive/Outils_Modoap/Detection_Illustrations/Poids* (seul le dernier fichier h5 généré sera réutilisé par la suite)\n",
        "\n",
        "Le paramètre --weights= permet de spécifier les poids initiaux à utiliser pour l'entraînement. Les valeurs possibles sont COCO, IMAGENET ou \"/chemin/vers/fichier.h5\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPLBPo9pO3i0"
      },
      "source": [
        " %cd $dossier_corpus\n",
        "!python /root/Mask_RCNN/illustration.py train --dataset=./ --weights=COCO --log=\"/content/drive/My Drive/Outils_Modoap/Detection_Illustrations/Poids\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}